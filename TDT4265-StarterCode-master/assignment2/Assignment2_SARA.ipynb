{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have this expression for updating the weight from one node in a layer to a node in the next layer.  \n",
    "$$\n",
    "w_{ji} := w_{ji} - \\alpha \\delta_j a_i\n",
    "$$\n",
    "\n",
    "Now we have to expand this to updating all the weights from all the nodes in the hidden layer to all the nodes in the output layer, and updating all the weights from the input layer to all the nodes in the hidden layer. \n",
    "\n",
    "First from the hidden layer to the output layer:\n",
    "\n",
    "$$\n",
    "w_{kj} := w_{kj} - \\alpha \\delta_k a_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta^K = -(Y_k - \\hat{Y}_k)\n",
    "$$\n",
    "\n",
    "Where $Y_k$ and $\\hat{Y}_k$ are the target output and the computed output respectivly. They are both vectores of dimension ten since we use ten output classes. \n",
    "\n",
    "Now for a hidden layer we use the backpropagation error equation we found in $1a$ that support multiple layers:\n",
    "$$\n",
    "\\delta_j^l = f'(z_j^l) \\sum_{k=1}^K w_{kj}^{l+1} \\delta_k^{l+1}\n",
    "$$\n",
    "In this equation we use $l$ and $l+1$ to implicate which layer the components belong to.\n",
    "The equation describes the backpropagation error equation for the $j_{th}$ node in layer $l$. The dimensions of $\\delta^l$ is $[J,1]$, where $J$ is the number of nodes in layer $l$. The dimensions of $\\delta^{l+1}$ is $[K,1]$, where $K$ is the number of nodes in layer $l+1$.\n",
    "\n",
    "Next, we want to calculate all the backpropagation errors for all the nodes in layer $l$ using matrix multiplication. By giving each $j$ a row in the vector representation, we can describe the weight matrix as $W$ where the weights from node $j$ is represented in row $j$ in $W$. Similarily, we create the matrix $\\Gamma'(z^l)$ with the values of $f'(z_j^l)$ along the diagonal, and all other values set to zero. \n",
    "The dimensions of $W$ is $[J,K]$, and the dimensions of $J$ is $[J,J]$\n",
    "\n",
    "Our equation for the backpropagation error now look like this:\n",
    "$$\n",
    "\\delta^l =  \\Gamma'(z^l)(W^{l+1})\\delta^{l+1}\n",
    "$$\n",
    "\n",
    "The new updating law is represented like this:\n",
    "$$\n",
    "W^l := W^l - \\alpha \\delta^l (a^{l-1})^{\\top}\n",
    "$$\n",
    "\n",
    "The updating law for the weights from the hidden layer to the output layer is represented as follows:\n",
    "$$\n",
    "W^k := W^k + \\alpha \\delta^K (a^{j}) = W^k + \\alpha (Y_k - \\hat{Y}_k) (a^{j})\n",
    "$$\n",
    "\n",
    "And the updating law for the weights from the input layer to the hidden layer is represented as:\n",
    "$$\n",
    "W^j := W^j - \\alpha \\delta^j (a^{i}) = W^j - \\alpha \\Gamma'(z^j)(W^{k})\\delta^{K} (a^{i}) = W^j + \\alpha \\Gamma'(z^j)(W^{k})(Y_k - \\hat{Y}_k)(a^{i})\n",
    "$$\n",
    "\n",
    "This is easy enough to implement in python using numphy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
