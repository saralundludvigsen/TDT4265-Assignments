{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - TDT4265\n",
    "#### Sara L. Ludvigsen and Emma H. BuÃ¸en\n",
    "##### April 2020\n",
    "\n",
    "## Task 1 - Softmax regression with backpropagation\n",
    "### 1.a - Backpropagation\n",
    "In this task we will show that \n",
    "$$\n",
    "w_{ji} := w_{ji} - \\alpha \\delta_j x_i\n",
    "$$\n",
    "and that\n",
    "$$\n",
    "\\delta_j = f'(z_j) \\sum_k w_{kj} \\delta_k .\n",
    "$$\n",
    "\n",
    "To do this, we first need to define our cost function as:\n",
    "$$\n",
    "C(w) = - \\frac{1}{N} \\sum_{n = 1}^{N} \\sum_{k = 1}^{K} y^n_k \\ln(\\hat{y}^n_k)\n",
    "$$\n",
    "\n",
    "We know from the problem description that the activation of hidden unit $j$ as $a_j = f(z_j)$, where f is the sigmoid function. It is also stated that\n",
    "$$\n",
    "z_j = \\sum_{i=0}^d w_{ji} x_i,\n",
    "$$\n",
    "and that $d$ is the dimensionality of the input. \n",
    "\n",
    "In the previous assignment, we showed that the update rule for for the weights $w_{kj}$ of the output\n",
    "layer is\n",
    "$$\n",
    "w_{kj} := w_{kj} - \\alpha \\delta_k a_j, \n",
    "$$\n",
    "where $\\delta_k = \\frac{\\partial C}{\\partial z_k} = -(y_k - \\hat{y}_k)$.\n",
    "\n",
    "Calculating the update rule for the weights $w_{ji}$:\n",
    "$$\n",
    "w_{ji} := w_{ji} - \\alpha \\frac{\\partial C}{\\partial w_{ij}} = w_{ji} - \\alpha \\overbrace{\\frac{\\partial C}{\\partial z_j}}^{\\delta_j} \\overbrace{\\frac{\\partial z_j}{\\partial w_{ij}}}^{x_i} = \\underline{w_{ji} - \\alpha \\delta_j x_i}\n",
    "$$\n",
    "$$\n",
    "\\delta_j = \\frac{\\partial C}{\\partial z_j} = \\frac{\\partial C}{\\partial a_j}\\frac{\\partial a_j}{\\partial z_j} = \\sum_k \\overbrace{\\frac{\\partial C}{\\partial z_k}}^{\\delta_k}  \\overbrace{\\frac{\\partial z_k}{\\partial a_j}}^{w_{kj}} \\overbrace{\\frac{\\partial a_j}{\\partial z_j}}^{f'(z_j)} = \\underline{f'(z_j) \\sum_k w_{kj} \\delta_k}\n",
    "$$\n",
    "\n",
    "\n",
    "### 1.b -  Vectorize computation\n",
    "We have this expression for updating the weight from one node in a layer to a node in the next layer.  \n",
    "$$\n",
    "w_{ji} := w_{ji} - \\alpha \\delta_j a_i\n",
    "$$\n",
    "\n",
    "Now we have to expand this to updating all the weights from all the nodes in the hidden layer to all the nodes in the output layer, and updating all the weights from the input layer to all the nodes in the hidden layer. \n",
    "\n",
    "First from the hidden layer to the output layer:\n",
    "\n",
    "$$\n",
    "w_{kj} := w_{kj} - \\alpha \\delta_k a_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta^K = -(Y_k - \\hat{Y}_k)\n",
    "$$\n",
    "\n",
    "Where $Y_k$ and $\\hat{Y}_k$ are the target output and the computed output respectivly. They are both vectores of dimension ten since we use ten output classes. \n",
    "\n",
    "Now for a hidden layer we use the backpropagation error equation we found in $1a$ that support multiple layers:\n",
    "$$\n",
    "\\delta_j^l = f'(z_j^l) \\sum_{k=1}^K w_{kj}^{l+1} \\delta_k^{l+1}\n",
    "$$\n",
    "In this equation we use $l$ and $l+1$ to implicate which layer the components belong to.\n",
    "The equation describes the backpropagation error equation for the $j_{th}$ node in layer $l$. The dimensions of $\\delta^l$ is $[J,1]$, where $J$ is the number of nodes in layer $l$. The dimensions of $\\delta^{l+1}$ is $[K,1]$, where $K$ is the number of nodes in layer $l+1$.\n",
    "\n",
    "Next, we want to calculate all the backpropagation errors for all the nodes in layer $l$ using matrix multiplication. By giving each $j$ a row in the vector representation, we can describe the weight matrix as $W$ where the weights from node $j$ is represented in row $j$ in $W$. Similarily, we create the matrix $\\Gamma'(z^l)$ with the values of $f'(z_j^l)$ along the diagonal, and all other values set to zero. \n",
    "The dimensions of $W$ is $[J,K]$, and the dimensions of $J$ is $[J,J]$\n",
    "\n",
    "Our equation for the backpropagation error now look like this:\n",
    "$$\n",
    "\\delta^l =  \\Gamma'(z^l)(W^{l+1})\\delta^{l+1}\n",
    "$$\n",
    "\n",
    "The new updating law is represented like this:\n",
    "$$\n",
    "W^l := W^l - \\alpha \\delta^l (a^{l-1})^{\\top}\n",
    "$$\n",
    "\n",
    "The updating law for the weights from the hidden layer to the output layer is represented as follows:\n",
    "$$\n",
    "W^k := W^k + \\alpha \\delta^K (a^{j}) = W^k + \\alpha (Y_k - \\hat{Y}_k) (a^{j})\n",
    "$$\n",
    "\n",
    "And the updating law for the weights from the input layer to the hidden layer is represented as:\n",
    "$$\n",
    "W^j := W^j - \\alpha \\delta^j (a^{i}) = W^j - \\alpha \\Gamma'(z^j)(W^{k})\\delta^{K} (a^{i}) = W^j + \\alpha \\Gamma'(z^j)(W^{k})(Y_k - \\hat{Y}_k)(a^{i})\n",
    "$$\n",
    "\n",
    "This is easy enough to implement in python using numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Softmax Regression with Backpropagation\n",
    "### 2.a - Preprocessing\n",
    "This is how we implemented the `pre_process_images` function. We modified it to have mean and standard deviation as parameters.\n",
    "\n",
    "```python\n",
    "def pre_process_images(X: np.ndarray, mean: float, std:float):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: images of shape [batch size, 784] in the range (0, 255)\n",
    "    Returns:\n",
    "        X: images of shape [batch size, 785]\n",
    "    \"\"\"\n",
    "    assert X.shape[1] == 784,\\\n",
    "        f\"X.shape[1]: {X.shape[1]}, should be 784\"\n",
    "\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    X = (X-(mean))/(std)\n",
    "    X = np.divide(X,255)\n",
    "    return X\n",
    "```\n",
    "\n",
    "We calculate the mean and std in `main` in `task2a.py`:\n",
    "```python\n",
    "    mean = np.mean(X_train)\n",
    "    std = np.std(X_train)\n",
    "    X_train = pre_process_images(X_train, mean, std)\n",
    "```\n",
    "\n",
    "This is how it processes images in `task2.py`, in its `main`function:\n",
    "```python\n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test = utils.load_full_mnist(validation_percentage)\n",
    "    mean = np.mean(X_train)\n",
    "    std = np.std(X_train)\n",
    "    X_train = pre_process_images(X_train, mean, std)\n",
    "    X_test = pre_process_images(X_test, mean, std)\n",
    "    X_val = pre_process_images(X_val, mean, std)\n",
    "    Y_train = one_hot_encode(Y_train, 10)\n",
    "    Y_val = one_hot_encode(Y_val, 10)\n",
    "    Y_test = one_hot_encode(Y_test, 10)\n",
    "```\n",
    "\n",
    "### 2.b - Softmax model\n",
    "The following code shows our implementation of `forward` and `backward` in the file `task2a.py`.\n",
    "\n",
    "```python\n",
    "def sigmoid(x: np.ndarray):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(a: np.ndarray):\n",
    "    a_exp = np.exp(a)\n",
    "    return a_exp / a_exp.sum(axis=1, keepdims=True)\n",
    "    \n",
    "class SoftmaxModel:\n",
    "\n",
    "    def __init__(self,\n",
    "                 # Number of neurons per layer\n",
    "                 neurons_per_layer: typing.List[int],\n",
    "                 use_improved_sigmoid: bool,  # Task 3a hyperparameter\n",
    "                 use_improved_weight_init: bool  # Task 3c hyperparameter\n",
    "                 ):\n",
    "        # Define number of input nodes\n",
    "        self.I = 785\n",
    "        self.use_improved_sigmoid = use_improved_sigmoid\n",
    "\n",
    "        # Define number of output nodes\n",
    "        # neurons_per_layer = [64, 10] indicates that we will have two layers:\n",
    "        # A hidden layer with 64 neurons and a output layer with 10 neurons.\n",
    "        self.neurons_per_layer = neurons_per_layer\n",
    "\n",
    "        # Initialize the weights\n",
    "        self.ws = []\n",
    "        prev = self.I\n",
    "        for size in self.neurons_per_layer:\n",
    "            w_shape = (prev, size)\n",
    "            print(\"Initializing weight to shape:\", w_shape)\n",
    "            w = np.random.uniform(-1,1,(w_shape))            \n",
    "            self.ws.append(w)\n",
    "            prev = size\n",
    "        self.grads = [0 for i in range(len(self.ws))]\n",
    "\n",
    "        #define a_i and a_j\n",
    "        self.z_j = []\n",
    "        self.a_j = []\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: images of shape [batch size, 785]\n",
    "        Returns:\n",
    "            y: output of model with shape [batch size, num_outputs]\n",
    "        \"\"\"\n",
    "        self.z_j = np.matmul(X,self.ws[0]) #(100,64)\n",
    "        self.a_j = sigmoid(self.z_j) #(100,64)\n",
    "        z_k = np.matmul(self.a_j,self.ws[1]) #(100,10)\n",
    "        output = softmax(z_k) #(100,10)\n",
    "        return output\n",
    "\n",
    "    def backward(self, X: np.ndarray, outputs: np.ndarray,\n",
    "                 targets: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: images of shape [batch size, 785]\n",
    "            outputs: outputs of model of shape: [batch size, num_outputs]\n",
    "            targets: labels/targets of each image of shape: [batch size, num_classes]\n",
    "        \"\"\"\n",
    "        assert targets.shape == outputs.shape,\\\n",
    "            f\"Output shape: {outputs.shape}, targets: {targets.shape}\"\n",
    "        # A list of gradients.\n",
    "        # For example, self.grads[0] will be the gradient for the first hidden layer\n",
    "\n",
    "        d_k = -(targets - outputs) #(100x10)\n",
    "        grad_1 = np.matmul(self.a_j.T, d_k)\n",
    "\n",
    "        df = self.a_j*(1-self.a_j) #(100x64)\n",
    "        temp = self.ws[1].dot(d_k.T) #(64x100)\n",
    "        d_j = df.T*temp #(64x100)\n",
    "        grad_0 = np.matmul(X.T, d_j.T)\n",
    "\n",
    "        self.grads = [grad_0, grad_1]\n",
    "\n",
    "\n",
    "        for grad, w in zip(self.grads, self.ws):\n",
    "            assert grad.shape == w.shape,\\\n",
    "                f\"Expected the same shape. Grad shape: {grad.shape}, w: {w.shape}.\"\n",
    "```\n",
    "\n",
    "### 2.c - Training\n",
    "![alt text](softmax_train_graph_2.png \"2c\")\n",
    "The plot to the left is the validation and training loss for every gradient step. The right plot is the training and validation accuracy. \n",
    "\n",
    "### 2.d - Parameters\n",
    "The number of parameters in our network is equal to the number of weights + the number of biases.\n",
    "$$number of parameters = 785*64 + 64*10 = 50 880$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - \"Tricks of the trade\"\n",
    "Without \"Tricks of the trade\"\n",
    "\n",
    "Metric | Value\n",
    "--- | ---\n",
    "Final Train Cross Entropy Loss| 0.031105040640134807\n",
    "Final Validation Cross Entropy Loss| 0.03486005051888923\n",
    "Final Test Cross Entropy Loss| 0.03211263794100177\n",
    "Final Train accuracy| 0.8949375\n",
    "Final Validation accuracy| 0.8888333333333334\n",
    "Final Test accuracy| 0.8932\n",
    "\n",
    "\n",
    "\n",
    "### 3.a - Shuffling\n",
    "We implemented \"Shuffling\" as follows:\n",
    "```python\n",
    "def shuffle_in_unison(a, b):\n",
    "    state = np.random.get_state()\n",
    "    a = np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    b = np.random.shuffle(b)\n",
    "    return a,b\n",
    "```\n",
    "With \"shuffling\", we got the following results:\n",
    "\n",
    "Metric | Value\n",
    "--- | ---\n",
    "Final Train Cross Entropy Loss| 0.02755075691377316\n",
    "Final Validation Cross Entropy Loss| 0.031061759331951252\n",
    "Final Test Cross Entropy Loss| 0.028824543997236746\n",
    "Final Train accuracy| 0.9101666666666667\n",
    "Final Validation accuracy| 0.8984166666666666\n",
    "Final Test accuracy| 0.9062\n",
    "\n",
    "We can observe a noticeable improvement in the test loss and all the accuracies. \n",
    "![alt text](A2_3a.png \"3a\")\n",
    "\n",
    "### 3.b - Improved Sigmoid\n",
    "This is our implementation of the improved sigmoid function:\n",
    "```python\n",
    "def improved_sigmoid(x: np.ndarray):\n",
    "    return 1.7159*np.tanh((2/3)*x)\n",
    "\n",
    "def improved_sigmoid_derivative(x: np.ndarray):\n",
    "    return 1.7159*2.0 / (3.0*(np.cosh(((2.0/3.0)*x))**2))\n",
    "```\n",
    "\n",
    "We got the following results:\n",
    "\n",
    "Metric | Value\n",
    "--- | ---\n",
    "Final Train Cross Entropy Loss| 0.02435081307283175\n",
    "Final Validation Cross Entropy Loss| 0.030451398557699797\n",
    "Final Test Cross Entropy Loss| 0.02835055746990178\n",
    "Final Train accuracy| 0.9205416666666667\n",
    "Final Validation accuracy| 0.90725\n",
    "Final Test accuracy| 0.9127\n",
    "\n",
    "![alt text](A2_3b.png \"3b\")\n",
    "\n",
    "### 3.c - Improved weight initialization\n",
    "The code in `task2a.py` where we initialize the weights:\n",
    "```python\n",
    "# Initialize the weights\n",
    "self.ws = []\n",
    "prev = self.I\n",
    "for size in self.neurons_per_layer:\n",
    "    w_shape = (prev, size)\n",
    "    print(\"Initializing weight to shape:\", w_shape)\n",
    "    if use_improved_weight_init == False:\n",
    "        w = np.random.uniform(-1,1,(w_shape))        \n",
    "    else: # Improved weight init\n",
    "        w = np.random.normal(0, 1/(np.sqrt(w_shape[1])), w_shape)    \n",
    "    self.ws.append(w)\n",
    "    prev = size\n",
    "self.grads = [0 for i in range(len(self.ws))]\n",
    "```\n",
    "\n",
    "Metric | Value\n",
    "--- | ---\n",
    "Final Train Cross Entropy Loss| 0.030247335380772573\n",
    "Final Validation Cross Entropy Loss| 0.03377519645466785\n",
    "Final Test Cross Entropy Loss| 0.03118569270086158\n",
    "Final Train accuracy| 0.89775\n",
    "Final Validation accuracy| 0.8916666666666667\n",
    "Final Test accuracy| 0.8955\n",
    "\n",
    "![alt text](A2_3c.png \"3c\")\n",
    "\n",
    "### 3.d - Momentum\n",
    "In the `train()` function i `task2.py`:\n",
    "```python\n",
    "    momentum = [0 for i in range(len(model.grads))]\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffling before next epoch\n",
    "        if use_shuffle == True:\n",
    "            shuffle_in_unison(X_train, Y_train)\n",
    "        for step in range(num_batches_per_epoch):\n",
    "            start = step * batch_size\n",
    "            end = start + batch_size\n",
    "            X_batch, Y_batch = X_train[start:end], Y_train[start:end]\n",
    "\n",
    "            y_hat = model.forward(X_batch)\n",
    "            model.backward(X_batch, y_hat, Y_batch)\n",
    "\n",
    "            if use_momentum == True:\n",
    "                momentum[0] = (1-momentum_gamma)*model.grads[0] + momentum_gamma*momentum[0]\n",
    "                momentum[1] = (1-momentum_gamma)*model.grads[1] + momentum_gamma*momentum[1]\n",
    "                model.ws[0] += -1*learning_rate*(momentum[0])\n",
    "                model.ws[1] += -1*learning_rate*(momentum[1])\n",
    "\n",
    "                '''\n",
    "                momentum(k+1) = gamma*momentum(k) + (1-gamma)*model.grads(k)\n",
    "                '''\n",
    "            else:\n",
    "                model.ws[0] += -1*learning_rate*model.grads[0]\n",
    "                model.ws[1] += -1*learning_rate*model.grads[1]\n",
    "```\n",
    "\n",
    "We got the following results with momentum:\n",
    "\n",
    "Metric | Value\n",
    "--- | ---\n",
    "Final Train Cross Entropy Loss| 0.028527996874256067\n",
    "Final Validation Cross Entropy Loss| 0.031918324737347306\n",
    "Final Test Cross Entropy Loss| 0.029540150765479535\n",
    "Final Train accuracy| 0.9071041666666667\n",
    "Final Validation accuracy| 0.8990833333333333\n",
    "Final Test accuracy| 0.9051\n",
    "\n",
    "![alt text](A2_3d.png \"3d\")\n",
    "\n",
    "### 3.e - Comparision\n",
    "If we use all the \"tricks of the trade\", we get these results:\n",
    "\n",
    "Metric | Value\n",
    "--- | ---\n",
    "Final Train Cross Entropy Loss| 0.024069764656181148\n",
    "Final Validation Cross Entropy Loss| 0.02961587176035768\n",
    "Final Test Cross Entropy Loss| 0.0275746366585815\n",
    "Final Train accuracy| 0.9216458333333334\n",
    "Final Validation accuracy| 0.9075\n",
    "Final Test accuracy| 0.9151\n",
    "\n",
    "![alt text](A2_3e.png \"3e\")\n",
    "\n",
    "#### Comparision, rounded to four decimals:\n",
    "\n",
    "Metric | Without tricks | With shuffling |Improved sigmoid|Improved weights | Momentum|All improvements\n",
    "--- | --- | ---| ---| ---| ---| ---\n",
    "Final Train Cross Entropy Loss| 0.0311|0.0276|0.0244|0.0302|0.0285|0.0241\n",
    "Final Validation Cross Entropy Loss| 0.0349|0.0311|0.0305|0.0338|0.0319|0.0296\n",
    "Final Test Cross Entropy Loss| 0.0321|0.0288|0.0284|0.0312|0.0295|0.0276\n",
    "Final Train accuracy| 0.8949|0.9102|0.9205|0.8978|0.9071|0.9216\n",
    "Final Validation accuracy| 0.8888|0.8984|0.9073|0.8917|0.8991|0.9075\n",
    "Final Test accuracy| 0.8932|0.9062|0.9127|0.8955|0.9051|0.9151\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
