{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - TDT4265\n",
    "#### Sara L. Ludvigsen and Emma H. Buøen\n",
    "##### April 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "### 1.a\n",
    "The intersection over union (IoU), is the ratio between the intersected area over the joined area for two regions.\n",
    "![IoU.png](IoU.png)\n",
    "\n",
    "### 1.b\n",
    "Precison measures how accurate your predictions is. Recall measures how good you find all the positives. When we say \"true positive\" we mean the cases where we predicted \"positive\" to be \"true\", and the prediction was correct. \"False positive\" is when we predict \"positive\", but it is incorrect. \n",
    "\n",
    "$$\n",
    "TP = True Positive\n",
    "$$\n",
    "$$\n",
    "TN = True Negative\n",
    "$$\n",
    "$$\n",
    "FP = False Positive\n",
    "$$\n",
    "$$\n",
    "FN = False Negative\n",
    "$$\n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "### 1.c\n",
    "\n",
    "Average precision (AP) is equal to the area under the precision and recall curve. The mean average precision (mAP) is the mean of the curves. \n",
    "\n",
    "*Insert drawing of precision and recall curve*\n",
    "\n",
    "$$\n",
    "AP_{class1} = 0.4*1 + 0.3*0.5 + 0.3*0.2 + 0.3*0.5*0.5 + 0.3*0.3*0.5 = 0.73\n",
    "$$\n",
    "\n",
    "$$\n",
    "AP_{class2} = 0.3*1 + 0.2*0.6 + 0.2*0.5 + 0.3*0.2 + 0.2*0.4*0.5 + 0.2*0.1*0.5 + 0.3*0.3*0.5 = 0.675\n",
    "$$\n",
    "\n",
    "$$\n",
    "mAP = \\frac{AP_{class1} + AP_{class2}}{2} = \\frac{0.73 + 0.675}{2} = 0.7025\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "### 2.a - 2.e \n",
    "was implemented in file `task2.py`.\n",
    "\n",
    "### 2.f\n",
    "![precision_recall_curve.png](task2/precision_recall_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "### 3.a\n",
    "*The SSD architecture produces a fixed-size number of bounding boxes and a score for each bounding box. After this, the final step is to filter out overlapping boxes, what is this called?*\n",
    "\n",
    "The final step to filter out overlapping boxes is called a non-maximum suppression step.\n",
    "\n",
    "### 3.b\n",
    "*The SSD architecture predicts bounding boxes at multiple scales to enable the network to detect objects of different sizes. Is the following true or false: Predictions from the deeper layers in SSD are responsible to detect small objects.*\n",
    "\n",
    "The statement \n",
    "\n",
    "> Predictions from the deeper layers in SSD are responsible to detect small objects.\n",
    "\n",
    "is false. The deeper layers are reponsible to detect larger objects. As stated in the [blog post](https://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06) about SSDs \n",
    "\n",
    "> Higher resolution feature maps are responsible for detecting small objects\n",
    "\n",
    "and the resoultion is highest at the beginning of the network, and is reduced for each layer.\n",
    "\n",
    "### 3.c\n",
    "\n",
    "*SSD use k number of bounding boxes with different aspect ratios at each spatial location in a feature map to predict c class scores and 4 offsets relative to the original box shape. Why do they use different bounding box aspect ratios at the same spatial location?Why do they use different bounding box aspect ratios at the same spatial location?*\n",
    "\n",
    "They use diffrent aspect ratios for bounding boxes at the same spatial locations so they get a diverse set of predictions, because it covers various input object shapes and sizes. They also observed that more default boxes are better. When they use 6 boxes per location, the performance is 0.6% better than with 4 boxes. 4 boxes is 2.1% better than 2 boxes. It seems that using a variety of default box shapes makes the prediction easier for the network.\n",
    "\n",
    "### 3.d\n",
    "*What is the main difference between SSD and YOLOv1/v2 (The YOLO version they refer to in the SSD paper)?*\n",
    "\n",
    "The main difference is that the SSD model adds several feature layers to the end of the network. These layers predict the offsets and confidences to default boxes of different scales and aspect ratios. Below is an image that illustrates the different network architectures.\n",
    "\n",
    "![Graphics/task3d_comparision.png](Graphics/task3d_comparision.png)\n",
    "\n",
    "The image is taken from [**SSD: Single Shot MultiBox Detector** by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg](https://arxiv.org/abs/1512.02325).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "### 4.a\n",
    "\n",
    "The code was implemented in `basic.py`.\n",
    "### 4.b\n",
    "```\n",
    "2020-04-14 21:35:58,315 SSD.trainer INFO: iter: 006000, lr: 0.00100, total_loss: 2.490 (3.411), reg_loss: 0.642 (0.846), cls_loss: 1.848 (2.566), time: 0.099 (0.097), eta: 3:03:26, mem: 2075M\n",
    "2020-04-14 21:35:58,343 SSD.trainer INFO: Saving checkpoint to outputs/basic/model_006000.pth\n",
    "2020-04-14 21:35:58,622 SSD.inference INFO: Evaluating mnist_detection_val dataset(1000 images):\n",
    "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 35.92it/s]\n",
    "2020-04-14 21:36:02,232 SSD.inference INFO: mAP: 0.7746\n",
    "```\n",
    "\n",
    "After 6000 iterations, our mean Average Precision was **77.46%**. \n",
    "\n",
    "Below is a plot of the total loss, taken from Tensorboard.\n",
    "\n",
    "![Graphics/task4b.png](Graphics/task4b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.c\n",
    "\n",
    "When changing the optimizer to Adam, and the learning rate to $2e-3$ and adding a BatchNorm2d layer to every feature bank, we achieved a mAP og **85.9%** after 9500 iterations!\n",
    "\n",
    "```\n",
    "2020-04-15 15:32:15,736 SSD.trainer INFO: iter: 009500, lr: 0.00020, total_loss: 1.696 (2.280), reg_loss: 0.434 (0.593), cls_loss: 1.262 (1.687), time: 0.099 (0.100), eta: 0:04:09, mem: 2075M\n",
    "2020-04-15 15:32:15,757 SSD.trainer INFO: Saving checkpoint to outputs/basic/model_009500.pth\n",
    "2020-04-15 15:32:16,145 SSD.inference INFO: Evaluating mnist_detection_val dataset(1000 images):\n",
    "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.23it/s]\n",
    "2020-04-15 15:32:19,983 SSD.inference INFO: mAP: 0.8589\n",
    "0               : 0.8779\n",
    "1               : 0.7953\n",
    "2               : 0.8640\n",
    "3               : 0.8828\n",
    "4               : 0.8665\n",
    "5               : 0.8716\n",
    "6               : 0.8695\n",
    "7               : 0.8428\n",
    "8               : 0.8803\n",
    "9               : 0.8379\n",
    "```\n",
    "\n",
    "This is the new and improved network structure:\n",
    "\n",
    "```python\n",
    "self.bank1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=image_channels,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ), \n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2,\n",
    "                stride=2\n",
    "            ),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ), \n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2,\n",
    "                stride=2\n",
    "            ),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ), \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=output_channels[0],\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            )\n",
    "        )\n",
    "        self.bank2 = nn.Sequential(\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(\n",
    "                in_channels=output_channels[0],\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(\n",
    "                in_channels=128,\n",
    "                out_channels=output_channels[1],\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            )\n",
    "        )\n",
    "        self.bank3 = nn.Sequential(\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(\n",
    "                in_channels=output_channels[1],\n",
    "                out_channels=256,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ), \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(\n",
    "                in_channels=256,\n",
    "                out_channels=output_channels[2],\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            )\n",
    "        )\n",
    "        self.bank4 = nn.Sequential(\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(\n",
    "                in_channels=output_channels[2],\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(\n",
    "                in_channels=128,\n",
    "                out_channels=output_channels[3],\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            )\n",
    "        )\n",
    "        self.bank5 = nn.Sequential(\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(\n",
    "                in_channels=output_channels[3],\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(\n",
    "                in_channels=128,\n",
    "                out_channels=output_channels[4],\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            )\n",
    "        )\n",
    "        self.bank6 = nn.Sequential(\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(\n",
    "                in_channels=output_channels[4],\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(\n",
    "                in_channels=128,\n",
    "                out_channels=output_channels[5],\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=0\n",
    "            )\n",
    "        )\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            self.bank1,\n",
    "            self.bank2,\n",
    "            self.bank3,\n",
    "            self.bank4,\n",
    "            self.bank5,\n",
    "            self.bank6\n",
    "        )\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.d\n",
    "\n",
    "In this task we changed the `MIN_SIZES` and `MAX_SIZES` for the model. We added the following to the `mnist.yaml` file:\n",
    "```python\n",
    "MIN_SIZES: [21, 45, 99, 153, 207, 261]\n",
    "MAX_SIZES: [45, 99, 153, 207, 261, 315]\n",
    "```\n",
    "```\n",
    "2020-04-15 20:48:43,930 SSD.trainer INFO: iter: 012500, lr: 0.00020, total_loss: 1.204 (1.731), reg_loss: 0.332 (0.438), cls_loss: 0.872 (1.293), time: 0.080 (0.097), eta: 0:07:17, mem: 2075M\n",
    "2020-04-15 20:48:43,961 SSD.trainer INFO: Saving checkpoint to outputs/basic/model_012500.pth\n",
    "2020-04-15 20:48:46,727 SSD.inference INFO: Evaluating mnist_detection_val dataset(1000 images):\n",
    "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.40it/s]\n",
    "2020-04-15 20:48:50,429 SSD.inference INFO: mAP: 0.9020\n",
    "0               : 0.9078\n",
    "1               : 0.8692\n",
    "2               : 0.9063\n",
    "3               : 0.9075\n",
    "4               : 0.9076\n",
    "5               : 0.9042\n",
    "6               : 0.9065\n",
    "7               : 0.9025\n",
    "8               : 0.9064\n",
    "9               : 0.9017\n",
    "```\n",
    "\n",
    "Iterations | mAP\n",
    "---|---\n",
    "12500|90.20%\n",
    "13000|90.02%\n",
    "13500|90.04%\n",
    "14000|90.12%\n",
    "14500|90.06%\n",
    "15000|90.20%\n",
    "\n",
    "We achieved these results by changing the config file `mnist.yaml` to the following:\n",
    "```python\n",
    "MODEL:\n",
    "    NUM_CLASSES: 11\n",
    "    BACKBONE:\n",
    "        NAME: 'basic'\n",
    "        PRETRAINED: False\n",
    "        OUT_CHANNELS: [128, 256, 128, 128, 64, 64]\n",
    "        INPUT_CHANNELS: 3\n",
    "    PRIORS:\n",
    "        FEATURE_MAPS: [38, 19, 10, 5, 3, 1]\n",
    "        STRIDES: [8, 16, 32, 64, 100, 300]\n",
    "        MIN_SIZES: [21, 45, 99, 153, 207, 261]\n",
    "        MAX_SIZES: [45, 99, 153, 207, 261, 315]\n",
    "        ASPECT_RATIOS: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\n",
    "        BOXES_PER_LOCATION: [4, 6, 6, 6, 4, 4]\n",
    "INPUT:\n",
    "    IMAGE_SIZE: 300\n",
    "DATASETS:\n",
    "    TRAIN: (\"mnist_detection_train\", \"mnist_detection_val\")\n",
    "    TEST: (\"mnist_detection_val\", )\n",
    "SOLVER:\n",
    "    MAX_ITER: 17000\n",
    "    LR_STEPS: [80000, 100000]\n",
    "    GAMMA: 0.1\n",
    "    BATCH_SIZE: 16\n",
    "    LR: 2e-4\n",
    "OUTPUT_DIR: 'outputs/basic'\n",
    "DATASET_DIR: \"datasets\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.e\n",
    "\n",
    "Here are the images from the demo-file:\n",
    "\n",
    "\n",
    "Image 0,3,6,9,12|Image 1,4,7,10,13|Image 2,5,8,11,14\n",
    ":--------------:|:---------------:|:----------------:\n",
    "![](Graphics/demo/download-0.png)  |  ![](Graphics/demo/download-1.png) |![](Graphics/demo/download-2.png) \n",
    "![](Graphics/demo/download-3.png)  |  ![](Graphics/demo/download-4.png) |![](Graphics/demo/download-5.png) \n",
    "![](Graphics/demo/download-6.png)  |  ![](Graphics/demo/download-7.png) |![](Graphics/demo/download-8.png) \n",
    "![](Graphics/demo/download-9.png)  |  ![](Graphics/demo/download-10.png) |![](Graphics/demo/download-11.png) \n",
    "![](Graphics/demo/download-12.png)  |  ![](Graphics/demo/download-13.png) |![](Graphics/demo/download-14.png) \n",
    "\n",
    "We observe that our model struggles the most with small-sized numbers and the number 1. Sometimes it also misses overlapping numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.f\n",
    "\n",
    "```\n",
    "2020-04-15 23:06:37,770 SSD.trainer INFO: iter: 005000, lr: 0.00050, total_loss: 4.660 (5.513), reg_loss: 1.320 (1.660), cls_loss: 3.340 (3.854), time: 0.799 (0.860), eta: 10:45:04, mem: 11167M\n",
    "2020-04-15 23:06:38,069 SSD.trainer INFO: Saving checkpoint to outputs/vgg_VOC/model_005000.pth\n",
    "2020-04-15 23:06:38,378 SSD.inference INFO: Evaluating voc_2007_test dataset(4952 images):\n",
    "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 496/496 [03:23<00:00,  3.09it/s]\n",
    "2020-04-15 23:10:05,051 SSD.inference INFO: mAP: 0.3288\n",
    "aeroplane       : 0.4453\n",
    "bicycle         : 0.3762\n",
    "bird            : 0.1836\n",
    "boat            : 0.1761\n",
    "bottle          : 0.0914\n",
    "bus             : 0.3907\n",
    "car             : 0.5665\n",
    "cat             : 0.4115\n",
    "chair           : 0.1784\n",
    "cow             : 0.3674\n",
    "diningtable     : 0.2833\n",
    "dog             : 0.2655\n",
    "horse           : 0.4536\n",
    "motorbike       : 0.3886\n",
    "person          : 0.5356\n",
    "pottedplant     : 0.0878\n",
    "sheep           : 0.2822\n",
    "sofa            : 0.2841\n",
    "train           : 0.3794\n",
    "tvmonitor       : 0.4291\n",
    "```\n",
    "\n",
    "![](Graphics/demo/download-15.png)  \n",
    "![](Graphics/demo/download-16.png) \n",
    "![](Graphics/demo/download-17.png) \n",
    "![](Graphics/demo/download-18.png)  \n",
    "![](Graphics/demo/download.png) \n",
    "\n",
    "![](Graphics/demo/total_plot.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
