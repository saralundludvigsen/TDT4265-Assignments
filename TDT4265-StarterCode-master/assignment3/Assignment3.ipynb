{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - TDT4265\n",
    "#### Sara L. Ludvigsen and Emma H. BuÃ¸en\n",
    "##### April 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Theory\n",
    "### 1.a \n",
    "In order to achieve a $3 \\times 5$ convolved image, we need to add a padding equal one to the input image.\n",
    "\n",
    "Input image ($3 \\times 5$):\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "3 & 0 & 1 & 7 & 3\\\\\n",
    "6 & 4 & 0 & 4 & 5\\\\\n",
    "4 & 6 & 3 & 2 & 4\\\\\n",
    "\\end{bmatrix}\n",
    "\\quad $ \n",
    "Padded input image: \n",
    "$\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 3 & 0 & 1 & 7 & 3 & 0 \\\\\n",
    "0 & 6 & 4 & 0 & 4 & 5 & 0 \\\\\n",
    "0 & 4 & 6 & 3 & 2 & 4 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\quad$\n",
    "Convolutional kernel ($3 \\times 3$ ): $\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0\\\\\n",
    "1 & -4 & 1\\\\\n",
    "0 & 1 & 0\\\\\n",
    "\\end{bmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spatial convolution is shown below:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "-4\\cdot 3+6 & 3+1+4 & -4+7 & 1-4\\cdot7+3+4 & 7-4\\cdot3+5\\\\\n",
    "3-4\\cdot6+4+4 & 6-4\\cdot4+6 & 1+4+4+3 & 7-4\\cdot4+5+2 & 3+4-4\\cdot5+4\\\\\n",
    "6-4\\cdot4+6 & 4+4-4\\cdot6+3 & 6-4\\cdot3+2 & 4+3-4\\cdot2+4 & 5+2-4\\cdot4\\\\\n",
    "\\end{bmatrix}\n",
    "\\quad  = \\quad\n",
    "\\begin{bmatrix}\n",
    "-6 & 8 & 3 & -20 & 0\\\\\n",
    "-13 & -4 & 12 & -2 & -9\\\\\n",
    "-4 & -13 & -4 & 3 & -9\\\\\n",
    "\\end{bmatrix}\n",
    "\\quad $ \n",
    "\n",
    "[Source](http://www.songho.ca/dsp/convolution/convolution2d_example.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b\n",
    "$(iii)$ The max pooling layer reduces the sensitivity to translationial variations in the input.\n",
    "\n",
    "### 1.c\n",
    "$$S_W = S_H =  1 \\qquad F_W = F_H = 5 \\qquad W_2 = W_1 \\qquad H_2 = H_1 \\qquad P_W = \\;? \\qquad P_H = \\;? $$\n",
    "\n",
    "$$W_2 = \\frac{W_1 - F_W + 2P_W}{S_W} +1 \\qquad \\qquad H_2 = \\frac{H_1 - F_H + 2P_H}{S_H} +1$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "(W-1)\\cdot S_W = W - F_W + 2P_W \\qquad \\qquad (H-1)\\cdot S_H = H - F_H + 2P_H\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_W = \\frac{(W-1)\\cdot S_W - W + F_W}{2} \\qquad \\qquad P_H = \\frac{(H-1)\\cdot S_H - H + F_H}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_W = \\frac{(W-1)\\cdot 1 - W + 5}{2} \\qquad \\qquad P_H = \\frac{(H-1)\\cdot 1 - H + 5}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_W = \\frac{W - 1 - W + 5}{2} \\qquad \\qquad P_H = \\frac{H - 1 - H + 5}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_W = \\frac{- 1 + 5}{2} = 1 \\qquad \\qquad P_H = \\frac{- 1 + 5}{2} = 1\n",
    "$$\n",
    "\n",
    "You should use a padding $ = 1$ on both sides if you want the shape of the output to be the same size as the input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.d\n",
    "$$\n",
    "W_1 = H_1 = 512 \\qquad C_1 = 3 \\qquad P_W = P_H = 0 \\qquad S_W = S_H = 1 \\qquad W_2 = H_2 = 504\n",
    "$$\n",
    "We want to find $F_W$ and $F_H$. Because $W = H$ for this task, we use the notation $X$.\n",
    "\n",
    "$$\n",
    "X_2 = \\frac{X_1 - F_X + 2P_X}{S_X} + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "504 = \\frac{512 - F_X + 0}{1} + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "F_X = 512 - 503 = 9\n",
    "$$\n",
    "\n",
    "The size of the kernels are $\\underline{(9 \\times 9)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.e \n",
    "$$\n",
    "W_1 = H_1 = 504 \\qquad C_1 = 12 \\qquad P_W = P_H = 0 \\qquad S_W = S_H = 2 \\qquad F_2 = F_2 = 2\n",
    "$$\n",
    "\n",
    "Our goal is to find W_2 and H_2.\n",
    "\n",
    "$$\n",
    "X_2 = \\frac{X_1 - F_X + 2P_X}{S_X} + 1 = \\frac{504 - 2}{2} +1 = 252\n",
    "$$\n",
    "\n",
    "The spatial dimensions of the pooled feature maps in the first pooling layer are $\\underline{(252 \\times 252)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.f\n",
    "\n",
    "For the convolutional layers, each feature extractor uses shared weights for its neurons.\n",
    "\n",
    "This means that the number of weights for each convolutional layer is equal to\n",
    "$$\n",
    "N_{weights} = F_H \\times F_W \\times C_1 \\times C_2\n",
    "$$\n",
    "where $F_H \\times F_W$ is our receptive field dimensions, and $C_1, C_2$ is the number of input channels and output channels (number of filters), respectively.\n",
    "\n",
    "Each output filter has one bias, so the number of parameters is then,\n",
    "$$\n",
    "N_{param} = N_{weights} + C_2\n",
    "$$\n",
    "\n",
    "For the fully connected layers, the number of weights is the product of the input dimension and the output dimension. So to determine the amount of weights for the first fully connected layer, we need to know the number of outputs from the third pooling layer. This will be equal to the number of outputs per feature extractor, multiplied by the number of feature extractors - i.e. $W_n \\cdot H_n \\cdot C_n $.\n",
    "\n",
    "Since \n",
    "$$\n",
    "W_{i+1} = (W_{i} - F_{W,i} + 2P_{W,i}) / S_{W,i} + 1\n",
    "$$\n",
    "$$\n",
    "H_{i+1} = (H_{i} - F_{H,i} + 2P_{H,i}) / S_{H,i} + 1\n",
    "$$\n",
    "\n",
    "For the first convolutional layer, where the input dimensions are the image dimensions: $32 \\times 32$, we get:\n",
    "$$\n",
    "W_1 = (32 - 5 + 2 \\cdot 2) / 1 + 1 = 32\n",
    "$$\n",
    "$$\n",
    "H_1 = (32 - 5 + 2 \\cdot 2) / 1 + 1 = 32\n",
    "$$\n",
    "And after the first maxpool layer, we get,\n",
    "$$\n",
    "W_2 = (32 - 2) / 2 + 1 = 16\n",
    "$$\n",
    "$$\n",
    "H_2 = (32 - 2) / 2 + 1 = 16\n",
    "$$\n",
    "Continuing this, we get\n",
    "$$\n",
    "W_3 = (16 - 5 + 2 \\cdot 2) / 1 + 1 = 16\n",
    "$$\n",
    "$$\n",
    "H_3 = (16 - 5 + 2 \\cdot 2) / 1 + 1 = 16\n",
    "$$\n",
    "$$\n",
    "W_4 = (16 - 2) / 2 + 1 = 8\n",
    "$$\n",
    "$$\n",
    "H_4 = (16 - 2) / 2 + 1 = 8\n",
    "$$\n",
    "$$\n",
    "W_5 = (8 - 5 + 2 \\cdot 2) / 1 + 1 = 8\n",
    "$$\n",
    "$$\n",
    "H_5 = (8 - 5 + 2 \\cdot 2) / 1 + 1 = 8\n",
    "$$\n",
    "$$\n",
    "W_6 = (16 - 2) / 2 + 1 = 4\n",
    "$$\n",
    "$$\n",
    "H_6 = (16 - 2) / 2 + 1 = 4\n",
    "$$\n",
    "\n",
    "Since we have $128$ output channels from the third pooling layer, and $4 \\cdot 4$ output features per feature extractor (channel), we thus get a total of $128 \\cdot 16 = 2048$ output features from the convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the formula for the total number of parameters, for all the layers, we get:\n",
    "\\begin{align}\n",
    "\\text{Conv Layer 1:  } &\\ \\   5 \\times 5 \\times 3 \\times 32 + 32 = & 2432  \\\\\n",
    "\\text{Conv Layer 2:  } &\\ \\   5 \\times 5 \\times 32 \\times 64 + 64 = & 51264  \\\\\n",
    "\\text{Conv Layer 3:  } &\\ \\   5 \\times 5 \\times 64 \\times 128 + 128 = & 204928  \\\\\n",
    "\\text{Connected Layer 1:  } &\\ \\   2048 \\times 64 + 64 = & 131136  \\\\\n",
    "\\text{Connected Layer 2:  } &\\ \\    64 \\times 10 + 10 = & 650  \\\\\n",
    "\\text{With a total number of parameters:  }& & 390410   \n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Convolutional Neural Networks\n",
    "### 2.a\n",
    "\n",
    "#### Implementation of `compute_loss_and_accuracy`:\n",
    "\n",
    "```python\n",
    "def compute_loss_and_accuracy(\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        model: torch.nn.Module,\n",
    "        loss_criterion: torch.nn.modules.loss._Loss):\n",
    "    \"\"\"\n",
    "    Computes the average loss and the accuracy over the whole dataset\n",
    "    in dataloader.\n",
    "    Args:\n",
    "        dataloder: Validation/Test dataloader\n",
    "        model: torch.nn.Module\n",
    "        loss_criterion: The loss criterion, e.g: torch.nn.CrossEntropyLoss()\n",
    "    Returns:\n",
    "        [average_loss, accuracy]: both scalar.\n",
    "    \"\"\"\n",
    "    average_loss = 0\n",
    "    accuracy = 0\n",
    "    total_images = 0\n",
    "    total_steps = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for (X_batch, Y_batch) in dataloader:\n",
    "            # Transfer images/labels to GPU VRAM, if possible\n",
    "            X_batch = utils.to_cuda(X_batch)\n",
    "            Y_batch = utils.to_cuda(Y_batch)\n",
    "            # Forward pass the images through our model\n",
    "            output_probs = model(X_batch)\n",
    "\n",
    "            # Compute Loss and Accuracy\n",
    "            loss = loss_criterion(output_probs, Y_batch) # tensor[value, grad_fn]\n",
    "\n",
    "            # From the example in pytorch docs:\n",
    "            _, predicted = torch.max(output_probs.data, 1)\n",
    "            \n",
    "            # Updating variables\n",
    "            total_images += Y_batch.size(0)\n",
    "            correct += (predicted == Y_batch).sum().item() \n",
    "            average_loss += loss.item()\n",
    "            total_steps += 1\n",
    "\n",
    "    average_loss = average_loss/total_steps\n",
    "    accuracy = correct/total_images\n",
    "\n",
    "    return average_loss, accuracy\n",
    "```\n",
    " \n",
    " #### `__init__` for `ExampleModel`:\n",
    " \n",
    " ```python\n",
    "class ExampleModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 image_channels,\n",
    "                 num_classes):\n",
    "        \"\"\"\n",
    "            Is called when model is initialized.\n",
    "            Args:\n",
    "                image_channels. Number of color channels in image (3)\n",
    "                num_classes: Number of classes we want to predict (10)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        num_filters = 32  # Set number of filters in first conv layer\n",
    "        self.num_classes = num_classes\n",
    "        # Define the convolutional layers\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=image_channels,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2,\n",
    "                stride=2\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                in_channels=32, \n",
    "                out_channels=64, \n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2,\n",
    "                stride=2\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                in_channels=64, \n",
    "                out_channels=128, \n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            )\n",
    "            , \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2,\n",
    "                stride=2\n",
    "            )\n",
    "        )\n",
    "        # The output of feature_extractor will be [batch_size, num_filters, 4, 4]\n",
    "        self.num_output_features = 2048 # = 128*4*4\n",
    "        # Initialize our last fully connected layer\n",
    "        # Inputs all extracted features from the convolutional layers\n",
    "        # Outputs num_classes predictions, 1 for each class.\n",
    "        # There is no need for softmax activation function, as this is\n",
    "        # included with nn.CrossEntropyLoss\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.num_output_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "```\n",
    " \n",
    "#### `forward` for `ExampleModel` :\n",
    " \n",
    "```python\n",
    "def forward(self, x):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through the model\n",
    "    Args:\n",
    "        x: Input image, shape: [batch_size, 3, 32, 32]\n",
    "    \"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    out = x\n",
    "    expected_shape = (batch_size, self.num_classes)\n",
    "    # Feature layers:\n",
    "    out = self.feature_extractor(out)\n",
    "    # Flattening:\n",
    "    out = out.view(-1, self.num_output_features)\n",
    "    # Fully-Connected layers\n",
    "    out = self.classifier(out)\n",
    "    assert out.shape == (batch_size, self.num_classes),\\\n",
    "        f\"Expected output of forward pass to be: {expected_shape}, but got: {out.shape}\"\n",
    "    return out\n",
    "```\n",
    "#### Results:\n",
    "When running `task2.py`we achieved a validation accuracy of $0.727$\n",
    "\n",
    "```\n",
    "Epoch:  9       Batches per seconds: 76.21      Global step:   6669     Validation Loss: 0.97,  Validation Accuracy: 0.727\n",
    "```\n",
    "\n",
    "The early stop criteria was met after 9 epochs. \n",
    "\n",
    "![task2_final.png](task2_final.png)\n",
    "\n",
    "The figure above shows the `cross entropy loss` and `accuracy` plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b\n",
    "The code had to be modified to include the computation of training accuracy. This is the modified `validation_epoch` function of the `Trainer` class.\n",
    "\n",
    "```python\n",
    "def validation_epoch(self):\n",
    "    \"\"\"\n",
    "        Computes the loss/accuracy for all three datasets.\n",
    "        Train, validation and test.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute for training set\n",
    "    _train_loss, train_acc = compute_loss_and_accuracy(\n",
    "        self.dataloader_train, self.model, self.loss_criterion\n",
    "    )\n",
    "    self.TRAIN_ACC[self.global_step] = train_acc\n",
    "\n",
    "    # Compute for validation set\n",
    "    self.model.eval()\n",
    "    validation_loss, validation_acc = compute_loss_and_accuracy(\n",
    "        self.dataloader_val, self.model, self.loss_criterion\n",
    "    )\n",
    "    self.VALIDATION_ACC[self.global_step] = validation_acc\n",
    "    self.VALIDATION_LOSS[self.global_step] = validation_loss\n",
    "    used_time = time.time() - self.start_time\n",
    "    print(\n",
    "        f\"Epoch: {self.epoch:>2}\",\n",
    "        f\"Batches per seconds: {self.global_step / used_time:.2f}\",\n",
    "        f\"Global step: {self.global_step:>6}\",\n",
    "        f\"Validation Loss: {validation_loss:.2f},\",\n",
    "        f\"Validation Accuracy: {validation_acc:.3f}\",\n",
    "        sep=\"\\t\")\n",
    "    # Compute for testing set\n",
    "    test_loss, test_acc = compute_loss_and_accuracy(\n",
    "        self.dataloader_test, self.model, self.loss_criterion\n",
    "    )\n",
    "    self.TEST_ACC[self.global_step] = test_acc\n",
    "    self.TEST_LOSS[self.global_step] = test_loss\n",
    "\n",
    "    self.model.train()\n",
    "```\n",
    "\n",
    "The results (rounded to three decimals) were :\n",
    "\n",
    "Dataset | Accuracy \n",
    "--- | --- \n",
    "Train | 0.780 \n",
    "Validation | 0.699\n",
    "Test | 0.694\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
