{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - TDT4265\n",
    "#### Sara L. Ludvigsen and Emma H. Buøen\n",
    "##### April 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Theory\n",
    "### 1.a \n",
    "In order to achieve a $3 \\times 5$ convolved image, we need to add a padding equal one to the input image.\n",
    "\n",
    "Input image ($3 \\times 5$):\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "3 & 0 & 1 & 7 & 3\\\\\n",
    "6 & 4 & 0 & 4 & 5\\\\\n",
    "4 & 6 & 3 & 2 & 4\\\\\n",
    "\\end{bmatrix}\n",
    "\\quad $ \n",
    "Padded input image: \n",
    "$\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 3 & 0 & 1 & 7 & 3 & 0 \\\\\n",
    "0 & 6 & 4 & 0 & 4 & 5 & 0 \\\\\n",
    "0 & 4 & 6 & 3 & 2 & 4 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\quad$\n",
    "Convolutional kernel ($3 \\times 3$ ): $\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0\\\\\n",
    "1 & -4 & 1\\\\\n",
    "0 & 1 & 0\\\\\n",
    "\\end{bmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spatial convolution is shown below:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "-4\\cdot 3+6 & 3+1+4 & -4+7 & 1-4\\cdot7+3+4 & 7-4\\cdot3+5\\\\\n",
    "3-4\\cdot6+4+4 & 6-4\\cdot4+6 & 1+4+4+3 & 7-4\\cdot4+5+2 & 3+4-4\\cdot5+4\\\\\n",
    "6-4\\cdot4+6 & 4+4-4\\cdot6+3 & 6-4\\cdot3+2 & 4+3-4\\cdot2+4 & 5+2-4\\cdot4\\\\\n",
    "\\end{bmatrix}\n",
    "\\quad  = \\quad\n",
    "\\begin{bmatrix}\n",
    "-6 & 8 & 3 & -20 & 0\\\\\n",
    "-13 & -4 & 12 & -2 & -9\\\\\n",
    "-4 & -13 & -4 & 3 & -9\\\\\n",
    "\\end{bmatrix}\n",
    "\\quad $ \n",
    "\n",
    "[Source](http://www.songho.ca/dsp/convolution/convolution2d_example.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b\n",
    "$(iii)$ The max pooling layer reduces the sensitivity to translationial variations in the input.\n",
    "\n",
    "### 1.c\n",
    "$$S_W = S_H =  1 \\qquad F_W = F_H = 5 \\qquad W_2 = W_1 \\qquad H_2 = H_1 \\qquad P_W = \\;? \\qquad P_H = \\;? $$\n",
    "\n",
    "$$W_2 = \\frac{W_1 - F_W + 2P_W}{S_W} +1 \\qquad \\qquad H_2 = \\frac{H_1 - F_H + 2P_H}{S_H} +1$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "(W-1)\\cdot S_W = W - F_W + 2P_W \\qquad \\qquad (H-1)\\cdot S_H = H - F_H + 2P_H\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_W = \\frac{(W-1)\\cdot S_W - W + F_W}{2} \\qquad \\qquad P_H = \\frac{(H-1)\\cdot S_H - H + F_H}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_W = \\frac{(W-1)\\cdot 1 - W + 5}{2} \\qquad \\qquad P_H = \\frac{(H-1)\\cdot 1 - H + 5}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_W = \\frac{W - 1 - W + 5}{2} \\qquad \\qquad P_H = \\frac{H - 1 - H + 5}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_W = \\frac{- 1 + 5}{2} = 1 \\qquad \\qquad P_H = \\frac{- 1 + 5}{2} = 1\n",
    "$$\n",
    "\n",
    "You should use a padding $ = 1$ on both sides if you want the shape of the output to be the same size as the input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.d\n",
    "$$\n",
    "W_1 = H_1 = 512 \\qquad C_1 = 3 \\qquad P_W = P_H = 0 \\qquad S_W = S_H = 1 \\qquad W_2 = H_2 = 504\n",
    "$$\n",
    "We want to find $F_W$ and $F_H$. Because $W = H$ for this task, we use the notation $X$.\n",
    "\n",
    "$$\n",
    "X_2 = \\frac{X_1 - F_X + 2P_X}{S_X} + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "504 = \\frac{512 - F_X + 0}{1} + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "F_X = 512 - 503 = 9\n",
    "$$\n",
    "\n",
    "The size of the kernels are $\\underline{(9 \\times 9)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.e \n",
    "$$\n",
    "W_1 = H_1 = 504 \\qquad C_1 = 12 \\qquad P_W = P_H = 0 \\qquad S_W = S_H = 2 \\qquad F_2 = F_2 = 2\n",
    "$$\n",
    "\n",
    "Our goal is to find $W_2$ and $H_2$.\n",
    "\n",
    "$$\n",
    "X_2 = \\frac{X_1 - F_X + 2P_X}{S_X} + 1 = \\frac{504 - 2}{2} +1 = 252\n",
    "$$\n",
    "\n",
    "The spatial dimensions of the pooled feature maps in the first pooling layer are $\\underline{(252 \\times 252)}$\n",
    " \n",
    "### 1.f\n",
    "\n",
    "$$\n",
    "X_2 = \\frac{X_1 - F_X + 2P_X}{S_X} + 1 = \\frac{252 - 3}{1} +1 = 250\n",
    "$$\n",
    "\n",
    "The spatial dimensions of the feature maps in the secind layer are $\\underline{(250 \\times 250)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.g\n",
    "For each convolutional layer, the number of weights is $N_w = F_H \\cdot F_W \\cdot C_1 \\cdot C_2$,\n",
    "and the number of biases is $N_b = C_2$. The total number of parameters is $N = (F_H \\cdot F_W \\cdot C_1 +1)\\cdot C_2$\n",
    "\n",
    "For the fully connected layers, the number of weights is $ N_w = dim(input) \\cdot dim(output)$. The dimension of the input in layer 4, is the dimension of the output of layer 3. Due to the flatten-layer, the dimension of the output of layer 3 is $W_i \\cdot H_i \\cdot C_i$. \n",
    "\n",
    "$$W_2 = \\frac{W_1 - F_W + 2P_W}{S_W} +1 \\qquad \\qquad H_2 = \\frac{H_1 - F_H + 2P_H}{S_H} +1$$\n",
    "\n",
    "For Conv2D:\n",
    "$$P_W = P_H = 2 \\qquad F_W = F_H = 5 \\qquad S_W = S_H = 1$$ \n",
    "For MaxPool2D:\n",
    "$$P_W = P_H = 0 \\qquad F_W = F_H = 2 \\qquad S_W = S_H = 2$$ \n",
    "\n",
    "Using the equations over, we get this table:\n",
    "\n",
    "\n",
    "Layer|Type|W|H\n",
    "---|---|---|---\n",
    "1|Conv2d|32|32\n",
    "1|MaxPool2d|16|16\n",
    "2|Conv2d|16|16\n",
    "2|MaxPool2d|8|8\n",
    "3|Conv2d|8|8\n",
    "3|MaxPool2d|4|4\n",
    "\n",
    "The dimension of the third layer is $W_i \\cdot H_i \\cdot C_i = 4 \\cdot 4 \\cdot 128 = 2048$\n",
    "\n",
    "The total number of parameters is **390 410**:\n",
    "\n",
    "Layer|Type|Formula&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; |Number of parameters\n",
    "---|---|---|---\n",
    "1|Conv2d|$(5\\cdot 5\\cdot3+1)32$|$2432$\n",
    "2|Conv2d|$(5\\cdot 5\\cdot32+1)64$|$51 264$\n",
    "3|Conv2d|$(5\\cdot 5\\cdot64+1)128$|$204 928$\n",
    "4|Fully-Connected|$(2048+1)64$|$131 136$\n",
    "5|Fully-Connected|$(64+1)10$|$650$\n",
    "-|Total||$390 410$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Convolutional Neural Networks\n",
    "### 2.a\n",
    "\n",
    "#### Implementation of `compute_loss_and_accuracy`:\n",
    "\n",
    "```python\n",
    "def compute_loss_and_accuracy(\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        model: torch.nn.Module,\n",
    "        loss_criterion: torch.nn.modules.loss._Loss):\n",
    "    \"\"\"\n",
    "    Computes the average loss and the accuracy over the whole dataset\n",
    "    in dataloader.\n",
    "    Args:\n",
    "        dataloder: Validation/Test dataloader\n",
    "        model: torch.nn.Module\n",
    "        loss_criterion: The loss criterion, e.g: torch.nn.CrossEntropyLoss()\n",
    "    Returns:\n",
    "        [average_loss, accuracy]: both scalar.\n",
    "    \"\"\"\n",
    "    average_loss = 0\n",
    "    accuracy = 0\n",
    "    total_images = 0\n",
    "    total_steps = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for (X_batch, Y_batch) in dataloader:\n",
    "            # Transfer images/labels to GPU VRAM, if possible\n",
    "            X_batch = utils.to_cuda(X_batch)\n",
    "            Y_batch = utils.to_cuda(Y_batch)\n",
    "            # Forward pass the images through our model\n",
    "            output_probs = model(X_batch)\n",
    "\n",
    "            # Compute Loss and Accuracy\n",
    "            loss = loss_criterion(output_probs, Y_batch) # tensor[value, grad_fn]\n",
    "\n",
    "            # From the example in pytorch docs:\n",
    "            _, predicted = torch.max(output_probs.data, 1)\n",
    "            \n",
    "            # Updating variables\n",
    "            total_images += Y_batch.size(0)\n",
    "            correct += (predicted == Y_batch).sum().item() \n",
    "            average_loss += loss.item()\n",
    "            total_steps += 1\n",
    "\n",
    "    average_loss = average_loss/total_steps\n",
    "    accuracy = correct/total_images\n",
    "\n",
    "    return average_loss, accuracy\n",
    "```\n",
    " \n",
    " #### `__init__` for `ExampleModel`:\n",
    " \n",
    " ```python\n",
    "class ExampleModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 image_channels,\n",
    "                 num_classes):\n",
    "        \"\"\"\n",
    "            Is called when model is initialized.\n",
    "            Args:\n",
    "                image_channels. Number of color channels in image (3)\n",
    "                num_classes: Number of classes we want to predict (10)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        num_filters = 32  # Set number of filters in first conv layer\n",
    "        self.num_classes = num_classes\n",
    "        # Define the convolutional layers\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=image_channels,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2,\n",
    "                stride=2\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                in_channels=32, \n",
    "                out_channels=64, \n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2,\n",
    "                stride=2\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                in_channels=64, \n",
    "                out_channels=128, \n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            )\n",
    "            , \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2,\n",
    "                stride=2\n",
    "            )\n",
    "        )\n",
    "        # The output of feature_extractor will be [batch_size, num_filters, 4, 4]\n",
    "        self.num_output_features = 2048 # = 128*4*4\n",
    "        # Initialize our last fully connected layer\n",
    "        # Inputs all extracted features from the convolutional layers\n",
    "        # Outputs num_classes predictions, 1 for each class.\n",
    "        # There is no need for softmax activation function, as this is\n",
    "        # included with nn.CrossEntropyLoss\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.num_output_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "```\n",
    " \n",
    "#### `forward` for `ExampleModel` :\n",
    " \n",
    "```python\n",
    "def forward(self, x):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through the model\n",
    "    Args:\n",
    "        x: Input image, shape: [batch_size, 3, 32, 32]\n",
    "    \"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    out = x\n",
    "    expected_shape = (batch_size, self.num_classes)\n",
    "    # Feature layers:\n",
    "    out = self.feature_extractor(out)\n",
    "    # Flattening:\n",
    "    out = out.view(-1, self.num_output_features)\n",
    "    # Fully-Connected layers\n",
    "    out = self.classifier(out)\n",
    "    assert out.shape == (batch_size, self.num_classes),\\\n",
    "        f\"Expected output of forward pass to be: {expected_shape}, but got: {out.shape}\"\n",
    "    return out\n",
    "```\n",
    "#### Results:\n",
    "When running `task2.py`we achieved a validation accuracy of $0.727$\n",
    "\n",
    "```\n",
    "Epoch:9   Batches per seconds:76.21   Global step:6669   Validation Loss:0.97   Validation Accuracy:0.727\n",
    "```\n",
    "\n",
    "The early stop criteria was met after 9 epochs. \n",
    "\n",
    "![task2_final.png](task2_final.png)\n",
    "\n",
    "The figure above shows the `cross entropy loss` and `accuracy` plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b\n",
    "The code had to be modified to include the computation of training accuracy. This is the modified `validation_epoch` function of the `Trainer` class.\n",
    "\n",
    "```python\n",
    "def validation_epoch(self):\n",
    "    \"\"\"\n",
    "        Computes the loss/accuracy for all three datasets.\n",
    "        Train, validation and test.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute for training set\n",
    "    _train_loss, train_acc = compute_loss_and_accuracy(\n",
    "        self.dataloader_train, self.model, self.loss_criterion\n",
    "    )\n",
    "    self.TRAIN_ACC[self.global_step] = train_acc\n",
    "\n",
    "    # Compute for validation set\n",
    "    self.model.eval()\n",
    "    validation_loss, validation_acc = compute_loss_and_accuracy(\n",
    "        self.dataloader_val, self.model, self.loss_criterion\n",
    "    )\n",
    "    self.VALIDATION_ACC[self.global_step] = validation_acc\n",
    "    self.VALIDATION_LOSS[self.global_step] = validation_loss\n",
    "    used_time = time.time() - self.start_time\n",
    "    print(\n",
    "        f\"Epoch: {self.epoch:>2}\",\n",
    "        f\"Batches per seconds: {self.global_step / used_time:.2f}\",\n",
    "        f\"Global step: {self.global_step:>6}\",\n",
    "        f\"Validation Loss: {validation_loss:.2f},\",\n",
    "        f\"Validation Accuracy: {validation_acc:.3f}\",\n",
    "        sep=\"\\t\")\n",
    "    # Compute for testing set\n",
    "    test_loss, test_acc = compute_loss_and_accuracy(\n",
    "        self.dataloader_test, self.model, self.loss_criterion\n",
    "    )\n",
    "    self.TEST_ACC[self.global_step] = test_acc\n",
    "    self.TEST_LOSS[self.global_step] = test_loss\n",
    "\n",
    "    self.model.train()\n",
    "```\n",
    "\n",
    "The results (rounded to three decimals) were :\n",
    "\n",
    "Dataset | Accuracy \n",
    "--- | --- \n",
    "Train | 0.780 \n",
    "Validation | 0.699\n",
    "Test | 0.694\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Deep Convolutional Network for Image Classification\n",
    "\n",
    "### 3.a\n",
    "\n",
    "#### Model 1\n",
    "In model 1 we have modified the **filter size** and the **Network architecture**. \n",
    "The kernel size is ($3 \\times 3$) for the convolution layers, and ($2 \\times 2$) for the pooling layers. Padding $=1$ for convolution layers. \n",
    "\n",
    "The new Network architecture is given in the table below. We tried to include \n",
    "\n",
    "\n",
    "    (conv-relu-conv-relu-pool)xN → (affine)xM → softmax and (batchnorm-relu-conv)xN → (affine)xM → softmax \n",
    "    \n",
    "    \n",
    "as suggested in the problem description.\n",
    "\n",
    "\n",
    "Layer Block |Layer| Layer Type | Number of Hidden Units / Units \n",
    "--- | --- | --- | --- \n",
    "1 |1| Conv2d  |in=3, out=32\n",
    "1 |2| BatchNorm2d  |32 \n",
    "1 |3| ReLu  | -\n",
    "1 |4| Conv2D  |in=32, out=64 \n",
    "1 |5| ReLu  | -\n",
    "1 |6| MaxPool2d  |stride=2\n",
    "2 |7| Conv2D  |in=64, out=128\n",
    "2 |8| BatchNorm2d  |128 \n",
    "2 |9| ReLu  | -\n",
    "2 |10| Conv2D  |in=128, out=128\n",
    "2 |11| ReLu  | -\n",
    "2 |12| MaxPool2d  |stride=2\n",
    "3 |13| Conv2D  |in=128, out=256\n",
    "3 |14| BatchNorm2d  |256\n",
    "3 |15| ReLu  | -\n",
    "3 |16| Conv2D  |in=256, out=256\n",
    "3 |17| ReLu  | -\n",
    "3 |18| MaxPool2d  |stride=2\n",
    "4 |19| Linear  |4096, 1024\n",
    "4 |20| ReLu  | -\n",
    "4 |21| Linear  |1024, 512\n",
    "4 |22| ReLu  | -\n",
    "4 |23| Linear  |512, 10\n",
    "\n",
    "The number of output features is 4096.\n",
    "\n",
    "Table | Training details\n",
    "--- | ---\n",
    "optimizer |  Stochastich Gradient Descent\n",
    "regularization | NA\n",
    "learning rate | 5e-2\n",
    "batch size | 64\n",
    "weight initialization |kaiming uniform\n",
    "activation function |ReLU\n",
    "\n",
    "Output from terminal from model 1:\n",
    "```\n",
    "Epoch:6   Batches per seconds:28.89   Global step:4563   Validation Loss:0.56   Validation Accuracy:0.814\n",
    "Early stop criteria met\n",
    "Early stopping.\n",
    "```\n",
    "\n",
    "**We achieved a validation accuracy of 81,4% after 6 epochs**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2\n",
    "In the second model the **Network architecture**, **Activation function**, **Filter size** and **Batch size** have changed. \n",
    "\n",
    "The table below illustrates the new Network architecture. The MaxPool2d filter has size ($2 \\times 2$), and the Conv2d filter has size ($5 \\times 5$) with padding$=2$. The ReLU activation function has been changed to the Exponential Linear Units (ELU). The Batch size is 128.\n",
    "\n",
    "Layer Block |Layer| Layer Type | Number of Hidden Units / Units \n",
    "--- | --- | --- | --- \n",
    "1 |1| Conv2d  |in=3, out=32\n",
    "1 |2| ELU  | -\n",
    "1 |3| Conv2D  |in=32, out=64 \n",
    "1 |4| ELU  | -\n",
    "1 |5| MaxPool2d  |stride=2\n",
    "2 |6| Conv2D  |in=64, out=128\n",
    "2 |7| ELU  | -\n",
    "2 |8| Conv2D  |in=128, out=128\n",
    "2 |9| ELU  | -\n",
    "2 |10| MaxPool2d  |stride=2\n",
    "2 |11| Dropout  |p=0.05\n",
    "3 |12| Conv2D  |in=128, out=256\n",
    "3 |13| ELU | -\n",
    "3 |14| Conv2D  |in=256, out=256\n",
    "3 |15| ELU  | -\n",
    "3 |16| MaxPool2d  |stride=2\n",
    "2 |17| Dropout  |p=0.1\n",
    "4 |18| Linear  |4096, 1024\n",
    "4 |19| ELU  | -\n",
    "4 |20| Linear  |1024, 512\n",
    "4 |21| ELU  | -\n",
    "2 |22| Dropout  |p=0.1\n",
    "4 |23| Linear  |512, 10\n",
    "\n",
    "\n",
    "The number of output features is 4096.\n",
    "\n",
    "Table | Training details\n",
    "--- | ---\n",
    "optimizer |  Stochastich Gradient Descent\n",
    "regularization | NA\n",
    "learning rate | 5e-2\n",
    "batch size | 128\n",
    "weight initialization |kaiming uniform\n",
    "activation function |ELU\n",
    "\n",
    "Output from terminal from model 2:\n",
    "```\n",
    "Epoch:9   Batches per seconds:12.31   Global step:3325   Validation Loss:0.71   Validation Accuracy:0.777\n",
    "Early stop criteria met\n",
    "Early stopping.\n",
    "```\n",
    "\n",
    "**We achieved a validation accuracy of 77,7% after 9 epochs**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b\n",
    "\n",
    "The table below is the comparision of the accuracies for the two models.\n",
    "\n",
    "Metric | Model 1 | Model 2\n",
    "---|---|---\n",
    "Train accuracy|0.9395|0.9153\n",
    "Validation accuracy|0.8142|0.7768\n",
    "Test accuracy|0.8163|0.7712\n",
    "Train loss|0.3138|0.3070\n",
    "Validation loss|0.5638|0.7076\n",
    "Test loss|0.5633|0.7198\n",
    "\n",
    "Model 1 is performing better than model 2. Below is the accuracy and loss plot for model 1.\n",
    "\n",
    "![task3b_model1.png](task3b_model1.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.c \n",
    "\n",
    "The method that had the most impact was to change the Network architecture. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
